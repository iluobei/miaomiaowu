package handler

import (
	"context"
	"errors"
	"fmt"
	"miaomiaowu/internal/logger"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"sort"
	"strconv"
	"strings"
	"time"

	"miaomiaowu/internal/auth"
	"miaomiaowu/internal/storage"
	"miaomiaowu/internal/substore"

	"gopkg.in/yaml.v3"
)

const subscriptionDefaultType = "clash"

// Tokenå¤±æ•ˆæ—¶è¿”å›çš„YAMLå†…å®¹
const tokenInvalidYAML = `allow-lan: false
dns:
  enable: true
  enhanced-mode: fake-ip
  ipv6: true
  nameserver:
    - https://120.53.53.53/dns-query
    - https://223.5.5.5/dns-query
  nameserver-policy:
    geosite:cn,private:
      - https://120.53.53.53/dns-query
      - https://223.5.5.5/dns-query
    geosite:geolocation-!cn:
      - https://dns.cloudflare.com/dns-query
      - https://dns.google/dns-query
  proxy-server-nameserver:
    - https://120.53.53.53/dns-query
    - https://223.5.5.5/dns-query
  respect-rules: true
geo-auto-update: true
geo-update-interval: 24
geodata-loader: standard
geodata-mode: true
geox-url:
  asn: https://github.com/xishang0128/geoip/releases/download/latest/GeoLite2-ASN.mmdb
  geoip: https://testingcf.jsdelivr.net/gh/MetaCubeX/meta-rules-dat@release/geoip.dat
  geosite: https://testingcf.jsdelivr.net/gh/MetaCubeX/meta-rules-dat@release/geosite.dat
  mmdb: https://testingcf.jsdelivr.net/gh/MetaCubeX/meta-rules-dat@release/country.mmdb
log-level: info
mode: rule
port: 7890
proxies:
  - name: âš ï¸ è®¢é˜…å·²è¿‡æœŸ
    type: ss
    server: test.example.com.cn
    port: 443
    password: J6h6sFZp0Xxv7M8K2RZ6nN8c8ZxQpJZcQ4M2YVtPZ5Q=
    cipher: 2022-blake3-chacha20-poly1305
  - name: âš ï¸ è¯·è”ç³»ç®¡ç†å‘˜
    type: ss
    server: test.example.com.cn
    port: 443
    password: J6h6sFZp0Xxv7M8K2RZ6nN8c8ZxQpJZcQ4M2YVtPZ5Q=
    cipher: 2022-blake3-chacha20-poly1305
proxy-groups:
  - name: ğŸš€ èŠ‚ç‚¹é€‰æ‹©
    type: select
    proxies:
      - âš ï¸ è®¢é˜…å·²è¿‡æœŸ
      - âš ï¸ è¯·è”ç³»ç®¡ç†å‘˜
rules:
  - MATCH,DIRECT
socks-port: 7891
`

const tokenInvalidFilename = "token_invalid.yaml"

// Context key for token invalid flag
type ContextKey string

const TokenInvalidKey ContextKey = "token_invalid"

type SubscriptionHandler struct {
	summary  *TrafficSummaryHandler
	repo     *storage.TrafficRepository
	baseDir  string
	fallback string
}

type subscriptionEndpoint struct {
	tokens *auth.TokenStore
	repo   *storage.TrafficRepository
	inner  *SubscriptionHandler
}

func NewSubscriptionHandler(repo *storage.TrafficRepository, baseDir string) http.Handler {
	if repo == nil {
		panic("subscription handler requires repository")
	}

	summary := NewTrafficSummaryHandler(repo)
	return newSubscriptionHandler(summary, repo, baseDir, subscriptionDefaultType)
}

// NewSubscriptionHandlerConcrete creates a subscription handler and returns the concrete type.
// This is used when other handlers need direct access to the SubscriptionHandler.
func NewSubscriptionHandlerConcrete(repo *storage.TrafficRepository, baseDir string) *SubscriptionHandler {
	if repo == nil {
		panic("subscription handler requires repository")
	}

	summary := NewTrafficSummaryHandler(repo)
	return newSubscriptionHandler(summary, repo, baseDir, subscriptionDefaultType)
}

// NewSubscriptionEndpoint returns a handler that serves subscription files, allowing either session tokens or user tokens via query parameter.
func NewSubscriptionEndpoint(tokens *auth.TokenStore, repo *storage.TrafficRepository, baseDir string) http.Handler {
	if tokens == nil {
		panic("subscription endpoint requires token store")
	}
	if repo == nil {
		panic("subscription endpoint requires repository")
	}

	inner := newSubscriptionHandler(nil, repo, baseDir, subscriptionDefaultType)
	return &subscriptionEndpoint{tokens: tokens, repo: repo, inner: inner}
}

func newSubscriptionHandler(summary *TrafficSummaryHandler, repo *storage.TrafficRepository, baseDir, fallback string) *SubscriptionHandler {
	if summary == nil {
		if repo == nil {
			panic("subscription handler requires repository")
		}
		summary = NewTrafficSummaryHandler(repo)
	}

	if repo == nil {
		panic("subscription handler requires repository")
	}

	if baseDir == "" {
		baseDir = filepath.FromSlash("subscribes")
	}

	cleanedBase := filepath.Clean(baseDir)
	if fallback == "" {
		fallback = subscriptionDefaultType
	}

	return &SubscriptionHandler{summary: summary, repo: repo, baseDir: cleanedBase, fallback: fallback}
}

func (s *subscriptionEndpoint) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	request, ok := s.authorizeRequest(w, r)
	if !ok {
		return
	}

	s.inner.ServeHTTP(w, request)
}

func (s *subscriptionEndpoint) authorizeRequest(w http.ResponseWriter, r *http.Request) (*http.Request, bool) {
	if r.Method != http.MethodGet {
		// allow handler to respond with method restrictions
		return r, true
	}

	// Check for username parameter (from composite short link - already authenticated by short link handler)
	queryUsername := strings.TrimSpace(r.URL.Query().Get("username"))
	if queryUsername != "" {
		ctx := auth.ContextWithUsername(r.Context(), queryUsername)
		return r.WithContext(ctx), true
	}

	// Check for token parameter (legacy/direct access)
	queryToken := strings.TrimSpace(r.URL.Query().Get("token"))
	if queryToken != "" && s.repo != nil {
		username, err := s.repo.ValidateUserToken(r.Context(), queryToken)
		if err == nil {
			ctx := auth.ContextWithUsername(r.Context(), username)
			return r.WithContext(ctx), true
		}
		if !errors.Is(err, storage.ErrTokenNotFound) {
			writeError(w, http.StatusInternalServerError, err)
			return nil, false
		}
	}

	// Check for header token (session-based access)
	headerToken := strings.TrimSpace(r.Header.Get(auth.AuthHeader))
	username, ok := s.tokens.Lookup(headerToken)
	if ok {
		ctx := auth.ContextWithUsername(r.Context(), username)
		return r.WithContext(ctx), true
	}

	// æ‰€æœ‰è®¤è¯æ–¹å¼éƒ½å¤±è´¥ï¼Œè®¾ç½®tokenå¤±æ•ˆæ ‡è®°
	ctx := context.WithValue(r.Context(), TokenInvalidKey, true)
	return r.WithContext(ctx), true
}

func (h *SubscriptionHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	// æ€§èƒ½ç›‘æµ‹ï¼šè®°å½•æ€»å¼€å§‹æ—¶é—´
	requestStart := time.Now()
	var stepStart time.Time

	if r.Method != http.MethodGet {
		writeError(w, http.StatusMethodNotAllowed, errors.New("only GET is supported"))
		return
	}

	// æ£€æŸ¥æ˜¯å¦æ˜¯tokenå¤±æ•ˆåœºæ™¯
	if tokenInvalid, ok := r.Context().Value(TokenInvalidKey).(bool); ok && tokenInvalid {
		h.serveTokenInvalidResponse(w, r)
		return
	}

	// Get username from context
	username := auth.UsernameFromContext(r.Context())

	// æ–‡ä»¶æŸ¥æ‰¾
	stepStart = time.Now()
	filename := strings.TrimSpace(r.URL.Query().Get("filename"))
	var subscribeFile storage.SubscribeFile
	var displayName string
	var err error
	var hasSubscribeFile bool

	if filename != "" {
		subscribeFile, err = h.repo.GetSubscribeFileByFilename(r.Context(), filename)
		if err != nil {
			if errors.Is(err, storage.ErrSubscribeFileNotFound) {
				writeError(w, http.StatusNotFound, errors.New("not found"))
				return
			}
			writeError(w, http.StatusInternalServerError, err)
			return
		}
		displayName = subscribeFile.Name
		hasSubscribeFile = true
	} else {
		// TODO: è®¢é˜…é“¾æ¥å·²ç»é…ç½®åˆ°å®¢æˆ·ç«¯ï¼Œç®¡ç†å‘˜ä¿®æ”¹æ–‡ä»¶ååï¼ŒåŸè®¢é˜…é“¾æ¥æ— æ³•ä½¿ç”¨
		// 1.0 ç‰ˆæœ¬æ—¶æ”¹ä¸ºä¸è¡¨é‡Œçš„IDå…³è”ï¼Œæš‚æ—¶å…ˆä¸æ”¹
		legacyName := strings.TrimSpace(r.URL.Query().Get("t"))
		link, err := h.resolveSubscription(r.Context(), legacyName)
		if err != nil {
			if errors.Is(err, storage.ErrSubscriptionNotFound) {
				writeError(w, http.StatusNotFound, err)
				return
			}
			writeError(w, http.StatusInternalServerError, err)
			return
		}
		filename = link.RuleFilename
		displayName = link.Name
		if h.repo != nil {
			subscribeFile, err = h.repo.GetSubscribeFileByFilename(r.Context(), filename)
			if err == nil {
				hasSubscribeFile = true
			} else if !errors.Is(err, storage.ErrSubscribeFileNotFound) {
				writeError(w, http.StatusInternalServerError, err)
				return
			}
		}
	}
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] æ–‡ä»¶æŸ¥æ‰¾å®Œæˆ", "step", "file_lookup", "duration_ms", time.Since(stepStart).Milliseconds(), "filename", filename)

	cleanedName := filepath.Clean(filename)
	if strings.HasPrefix(cleanedName, "..") || filepath.IsAbs(cleanedName) {
		writeError(w, http.StatusBadRequest, errors.New("invalid rule filename"))
		return
	}

	resolvedPath := filepath.Join(h.baseDir, cleanedName)

	// Verify resolved path is within baseDir to prevent path traversal
	absBase, err := filepath.Abs(h.baseDir)
	if err != nil {
		writeError(w, http.StatusInternalServerError, err)
		return
	}
	absResolved, err := filepath.Abs(resolvedPath)
	if err != nil {
		writeError(w, http.StatusInternalServerError, err)
		return
	}
	if !strings.HasPrefix(absResolved, absBase+string(filepath.Separator)) && absResolved != absBase {
		writeError(w, http.StatusBadRequest, errors.New("invalid rule filename"))
		return
	}

	if hasSubscribeFile && subscribeFile.ExpireAt != nil {
		now := time.Now()
		if !subscribeFile.ExpireAt.After(now) {
			logger.Info("[Subscription] è®¢é˜…å·²è¿‡æœŸ", "filename", filename, "expire_at", subscribeFile.ExpireAt.Format("2006-01-02 15:04:05"))
			h.serveTokenInvalidResponse(w, r)
			return
		}
	}

	// æ–‡ä»¶è¯»å–
	stepStart = time.Now()
	data, err := os.ReadFile(resolvedPath)
	if err != nil {
		if errors.Is(err, os.ErrNotExist) {
			writeError(w, http.StatusNotFound, err)
		} else {
			writeError(w, http.StatusInternalServerError, err)
		}
		return
	}
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] æ–‡ä»¶è¯»å–å®Œæˆ", "step", "file_read", "duration_ms", time.Since(stepStart).Milliseconds(), "bytes", len(data))

	// MMW åŒæ­¥
	stepStart = time.Now()
	// åŒæ­¥ MMW æ¨¡å¼ä»£ç†é›†åˆçš„èŠ‚ç‚¹åˆ°è®¢é˜…æ–‡ä»¶
	// è¿™æ ·å¯ä»¥ç¡®ä¿è·å–è®¢é˜…æ—¶åŒ…å«æœ€æ–°çš„ä»£ç†é›†åˆèŠ‚ç‚¹
	if h.repo != nil {
		SyncMMWProxyProvidersToFile(h.repo, h.baseDir, cleanedName)
		// é‡æ–°è¯»å–æ›´æ–°åçš„æ–‡ä»¶
		updatedData, err := os.ReadFile(resolvedPath)
		if err == nil {
			data = updatedData
		}
	}
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] MMW åŒæ­¥å®Œæˆ", "step", "mmw_sync", "duration_ms", time.Since(stepStart).Milliseconds())

	// å¤–éƒ¨è®¢é˜…åŒæ­¥
	stepStart = time.Now()
	// Check if force sync external subscriptions is enabled and sync only referenced subscriptions
	if username != "" && h.repo != nil {
		settings, err := h.repo.GetUserSettings(r.Context(), username)
		if err == nil && settings.ForceSyncExternal {
			logger.Info("[Subscription] ç”¨æˆ·å¯ç”¨å¼ºåˆ¶åŒæ­¥", "user", username, "cache_expire_minutes", settings.CacheExpireMinutes)

			// Get external subscriptions referenced in current file
			usedExternalSubs, err := GetExternalSubscriptionsFromFile(r.Context(), data, username, h.repo)
			if err != nil {
				logger.Info("[Subscription] è·å–æ–‡ä»¶ä¸­çš„å¤–éƒ¨è®¢é˜…å¤±è´¥", "error", err)
			} else if len(usedExternalSubs) > 0 {
				logger.Info("[Subscription] æ‰¾åˆ°å½“å‰æ–‡ä»¶å¼•ç”¨çš„å¤–éƒ¨è®¢é˜…", "count", len(usedExternalSubs))

				// Get user's external subscriptions to check cache and get URLs
				allExternalSubs, err := h.repo.ListExternalSubscriptions(r.Context(), username)
				if err != nil {
					logger.Info("[Subscription] è·å–å¤–éƒ¨è®¢é˜…åˆ—è¡¨å¤±è´¥", "error", err)
				} else {
					// Filter to only sync subscriptions that are referenced in the current file
					var subsToSync []storage.ExternalSubscription
					subURLMap := make(map[string]string) // URL -> name mapping

					for _, sub := range allExternalSubs {
						subURLMap[sub.URL] = sub.Name
						if _, used := usedExternalSubs[sub.URL]; used {
							subsToSync = append(subsToSync, sub)
						}
					}

					logger.Info("[Subscription] å¼ºåˆ¶åŒæ­¥å·²å¯ç”¨ï¼Œå°†åŒæ­¥å¼•ç”¨çš„å¤–éƒ¨è®¢é˜…", "sync_count", len(subsToSync), "total_count", len(allExternalSubs))

					// Check if we need to sync based on cache expiration
					shouldSync := false
					if settings.CacheExpireMinutes > 0 {
						// Check last sync time only for referenced subscriptions
						for _, sub := range subsToSync {
							if sub.LastSyncAt == nil {
								// Never synced before
								logger.Info("[Subscription] è®¢é˜…ä»æœªåŒæ­¥è¿‡ï¼Œå°†è¿›è¡ŒåŒæ­¥", "name", sub.Name, "url", sub.URL)
								shouldSync = true
								break
							}

							// Calculate time difference in minutes
							elapsed := time.Since(*sub.LastSyncAt).Minutes()
							if elapsed >= float64(settings.CacheExpireMinutes) {
								// Cache expired
								logger.Info("[Subscription] è®¢é˜…ç¼“å­˜å·²è¿‡æœŸï¼Œå°†è¿›è¡ŒåŒæ­¥", "name", sub.Name, "url", sub.URL, "elapsed_minutes", elapsed, "expire_minutes", settings.CacheExpireMinutes)
								shouldSync = true
								break
							}
						}
						if !shouldSync {
							logger.Info("[Subscription] All referenced subscriptions are within cache time, skipping sync")
						}
					} else {
						// Cache expire minutes is 0, always sync
						logger.Info("[Subscription] Cache expire minutes is 0, will always sync referenced subscriptions")
						shouldSync = true
					}

					if shouldSync {
						logger.Info("[Subscription] å¼€å§‹åŒæ­¥ç”¨æˆ·çš„å¤–éƒ¨è®¢é˜…(ä»…å¼•ç”¨çš„è®¢é˜…)", "user", username)
						// Sync only the referenced external subscriptions
						if err := syncReferencedExternalSubscriptions(r.Context(), h.repo, h.baseDir, username, subsToSync); err != nil {
							logger.Info("[Subscription] åŒæ­¥å¤–éƒ¨è®¢é˜…å¤±è´¥", "error", err)
							// Log error but don't fail the request
							// The sync is best-effort
						} else {
							logger.Info("[Subscription] External subscriptions sync completed successfully")

							// Re-read the subscription file after sync to get updated nodes
							updatedData, err := os.ReadFile(resolvedPath)
							if err != nil {
								logger.Info("[Subscription] åŒæ­¥åé‡æ–°è¯»å–è®¢é˜…æ–‡ä»¶å¤±è´¥", "error", err)
							} else {
								data = updatedData
								logger.Info("[Subscription] åŒæ­¥åé‡æ–°è¯»å–è®¢é˜…æ–‡ä»¶æˆåŠŸ", "bytes", len(data))
							}
						}
					}
				}
			} else {
				logger.Info("[Subscription] No external subscriptions referenced in current file, skipping sync")
			}
		}
	}
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] å¤–éƒ¨è®¢é˜…åŒæ­¥å®Œæˆ", "step", "external_sync", "duration_ms", time.Since(stepStart).Milliseconds())

	// æµé‡ä¿¡æ¯æ”¶é›†
	stepStart = time.Now()
	// åœ¨è½¬æ¢è®¢é˜…æ ¼å¼ä¹‹å‰ï¼Œå…ˆæ”¶é›†æ¢é’ˆæœåŠ¡å™¨å’Œå¤–éƒ¨è®¢é˜…æµé‡ä¿¡æ¯
	// è¿™æ ·å¯ä»¥ç¡®ä¿æ— è®ºè®¢é˜…è¢«è½¬æ¢æˆä»€ä¹ˆæ ¼å¼ï¼Œéƒ½èƒ½æ­£ç¡®æ”¶é›†ä¿¡æ¯
	externalTrafficLimit, externalTrafficUsed := int64(0), int64(0)
	usesProbeNodes := false                  // æ˜¯å¦ä½¿ç”¨äº†æ¢é’ˆèŠ‚ç‚¹
	probeBindingEnabled := false             // æ˜¯å¦å¼€å¯äº†æ¢é’ˆæœåŠ¡å™¨ç»‘å®š
	var usedProbeServers map[string]struct{} // è®¢é˜…æ–‡ä»¶ä¸­ä½¿ç”¨çš„æ¢é’ˆæœåŠ¡å™¨åˆ—è¡¨

	if username != "" && h.repo != nil {
		settings, err := h.repo.GetUserSettings(r.Context(), username)
		if err == nil {
			probeBindingEnabled = settings.EnableProbeBinding

			// å¦‚æœå¼€å¯äº†æ¢é’ˆç»‘å®šæˆ–æµé‡åŒæ­¥ï¼Œéœ€è¦è§£æ YAML è·å–èŠ‚ç‚¹ä¿¡æ¯
			if probeBindingEnabled || settings.SyncTraffic {
				// è§£æ YAML æ–‡ä»¶ï¼Œè·å–å…¶ä¸­ä½¿ç”¨çš„èŠ‚ç‚¹åç§°
				var yamlConfig map[string]any
				if err := yaml.Unmarshal(data, &yamlConfig); err == nil {
					if proxies, ok := yamlConfig["proxies"].([]any); ok {
						logger.Info("[Subscription] æ‰¾åˆ°è®¢é˜…YAMLä¸­çš„ä»£ç†èŠ‚ç‚¹", "count", len(proxies))
						// æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹åç§°
						usedNodeNames := make(map[string]bool)
						for _, proxy := range proxies {
							if proxyMap, ok := proxy.(map[string]any); ok {
								if name, ok := proxyMap["name"].(string); ok && name != "" {
									usedNodeNames[name] = true
								}
							}
						}

						// å¦‚æœæœ‰èŠ‚ç‚¹åç§°ï¼Œä»æ•°æ®åº“æŸ¥è¯¢è¿™äº›èŠ‚ç‚¹
						if len(usedNodeNames) > 0 {
							logger.Info("[Subscription] æŸ¥è¯¢æ•°æ®åº“ä¸­çš„èŠ‚ç‚¹", "count", len(usedNodeNames))
							nodes, err := h.repo.ListNodes(r.Context(), username)
							if err == nil {
								// æ”¶é›†ä½¿ç”¨åˆ°çš„å¤–éƒ¨è®¢é˜…åç§°ï¼ˆé€šè¿‡ tag è¯†åˆ«ï¼‰
								usedExternalSubs := make(map[string]bool)

								for _, node := range nodes {
									// æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦åœ¨è®¢é˜…æ–‡ä»¶ä¸­
									if usedNodeNames[node.NodeName] {
										// æ£€æµ‹æ˜¯å¦ä¸ºæ¢é’ˆèŠ‚ç‚¹ï¼ˆæœ‰ç»‘å®šæ¢é’ˆæœåŠ¡å™¨ï¼‰
										if probeBindingEnabled && node.ProbeServer != "" {
											usesProbeNodes = true
											// æ”¶é›†è®¢é˜…æ–‡ä»¶ä¸­ä½¿ç”¨çš„æ¢é’ˆæœåŠ¡å™¨
											if usedProbeServers == nil {
												usedProbeServers = make(map[string]struct{})
											}
											usedProbeServers[node.ProbeServer] = struct{}{}
											logger.Info("[Subscription] æ£€æµ‹åˆ°æ¢é’ˆèŠ‚ç‚¹ç»‘å®šæœåŠ¡å™¨", "node_name", node.NodeName, "probe_server", node.ProbeServer)
										}

										// å¦‚æœå¼€å¯äº†æµé‡åŒæ­¥ï¼Œæ”¶é›†å¤–éƒ¨è®¢é˜…èŠ‚ç‚¹
										if settings.SyncTraffic {
											// å¦‚æœ tag ä¸æ˜¯é»˜è®¤å€¼ï¼Œè¯´æ˜æ˜¯å¤–éƒ¨è®¢é˜…èŠ‚ç‚¹
											if node.Tag != "" && node.Tag != "æ‰‹åŠ¨è¾“å…¥" {
												usedExternalSubs[node.Tag] = true
												logger.Info("[Subscription] èŠ‚ç‚¹æ¥è‡ªå¤–éƒ¨è®¢é˜…", "node_name", node.NodeName, "tag", node.Tag)
											}
										}
									}
								}

								// å¦‚æœå¼€å¯äº†æµé‡åŒæ­¥ä¸”æœ‰ä½¿ç”¨åˆ°å¤–éƒ¨è®¢é˜…çš„èŠ‚ç‚¹ï¼Œæ±‡æ€»è¿™äº›è®¢é˜…çš„æµé‡
								if settings.SyncTraffic && len(usedExternalSubs) > 0 {
									logger.Info("[Subscription] ç”¨æˆ·å¯ç”¨æµé‡åŒæ­¥ï¼Œæ‰¾åˆ°ä½¿ç”¨ä¸­çš„å¤–éƒ¨è®¢é˜…", "user", username, "count", len(usedExternalSubs), "tags", getKeys(usedExternalSubs))
									externalSubs, err := h.repo.ListExternalSubscriptions(r.Context(), username)
									if err == nil {
										now := time.Now()
										for _, sub := range externalSubs {
											// åªæ±‡æ€»ä½¿ç”¨åˆ°çš„å¤–éƒ¨è®¢é˜…
											if usedExternalSubs[sub.Name] {
												// å¦‚æœæœ‰è¿‡æœŸæ—¶é—´ä¸”å·²è¿‡æœŸï¼Œåˆ™è·³è¿‡
												// å¦‚æœè¿‡æœŸæ—¶é—´ä¸ºç©ºï¼Œè¡¨ç¤ºé•¿æœŸè®¢é˜…ï¼Œä¸è·³è¿‡
												if sub.Expire != nil && sub.Expire.Before(now) {
													logger.Info("[Subscription] è·³è¿‡å·²è¿‡æœŸçš„å¤–éƒ¨è®¢é˜…", "name", sub.Name, "expire", sub.Expire.Format("2006-01-02 15:04:05"))
													continue
												}
												if sub.Expire == nil {
													logger.Info("[Subscription] æ·»åŠ é•¿æœŸå¤–éƒ¨è®¢é˜…æµé‡", "name", sub.Name, "upload", sub.Upload, "download", sub.Download, "total", sub.Total, "mode", sub.TrafficMode)
												} else {
													logger.Info("[Subscription] æ·»åŠ å¤–éƒ¨è®¢é˜…æµé‡", "name", sub.Name, "upload", sub.Upload, "download", sub.Download, "total", sub.Total, "mode", sub.TrafficMode, "expire", sub.Expire.Format("2006-01-02 15:04:05"))
												}
												externalTrafficLimit += sub.Total
												// æ ¹æ® TrafficMode è®¡ç®—å·²ç”¨æµé‡
												switch sub.TrafficMode {
												case "download":
													externalTrafficUsed += sub.Download
												case "upload":
													externalTrafficUsed += sub.Upload
												default: // "both" æˆ–ç©º
													externalTrafficUsed += sub.Upload + sub.Download
												}
											}
										}
										logger.Info("[Subscription] å¤–éƒ¨è®¢é˜…æµé‡æ±‡æ€»", "limit_bytes", externalTrafficLimit, "limit_gb", float64(externalTrafficLimit)/(1024*1024*1024), "used_bytes", externalTrafficUsed, "used_gb", float64(externalTrafficUsed)/(1024*1024*1024))
									} else {
										logger.Info("[Subscription] è·å–å¤–éƒ¨è®¢é˜…åˆ—è¡¨å¤±è´¥", "error", err)
									}
								} else if settings.SyncTraffic {
									logger.Info("[Subscription] ç”¨æˆ·å¯ç”¨æµé‡åŒæ­¥ä½†æœªæ‰¾åˆ°ä½¿ç”¨ä¸­çš„å¤–éƒ¨è®¢é˜…èŠ‚ç‚¹", "user", username)
								}
							} else {
								logger.Info("[Subscription] è·å–èŠ‚ç‚¹åˆ—è¡¨å¤±è´¥", "error", err)
							}
						}
					}
				}
			}
		}
	}
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] æµé‡ä¿¡æ¯æ”¶é›†å®Œæˆ", "step", "traffic_info", "duration_ms", time.Since(stepStart).Milliseconds())

	// èŠ‚ç‚¹æ’åº
	stepStart = time.Now()
	// è·å–ç”¨æˆ·çš„èŠ‚ç‚¹æ’åºé…ç½®ï¼Œéœ€è¦åœ¨è½¬æ¢ä¹‹å‰ä½¿ç”¨
	var nodeOrder []int64
	if username != "" && h.repo != nil {
		settings, err := h.repo.GetUserSettings(r.Context(), username)
		if err == nil {
			nodeOrder = settings.NodeOrder
			logger.Info("[Subscription] ç”¨æˆ·èŠ‚ç‚¹æ’åºé…ç½®", "user", username, "node_count", len(nodeOrder))
		}
	}

	// åœ¨è½¬æ¢ä¹‹å‰æ ¹æ®èŠ‚ç‚¹æ’åºé…ç½®è°ƒæ•´åŸå§‹ YAML
	// è¿™æ ·è½¬æ¢åçš„ä»»ä½•æ ¼å¼éƒ½ä¼šä¿æŒæ­£ç¡®çš„èŠ‚ç‚¹é¡ºåº
	if len(nodeOrder) > 0 && username != "" && h.repo != nil {
		var yamlNode yaml.Node
		if err := yaml.Unmarshal(data, &yamlNode); err == nil {
			shouldRewrite := false
			if len(yamlNode.Content) > 0 && yamlNode.Content[0].Kind == yaml.MappingNode {
				rootMap := yamlNode.Content[0]
				for i := 0; i < len(rootMap.Content); i += 2 {
					if rootMap.Content[i].Value == "proxies" {
						proxiesNode := rootMap.Content[i+1]
						if proxiesNode.Kind == yaml.SequenceNode {
							if err := sortProxiesByNodeOrder(r.Context(), h.repo, username, proxiesNode, nodeOrder); err != nil {
								logger.Info("[Subscription] è½¬æ¢å‰æŒ‰èŠ‚ç‚¹é¡ºåºæ’åºå¤±è´¥", "error", err)
							} else {
								shouldRewrite = true
								logger.Info("[Subscription] Successfully sorted proxies by node order before conversion")
							}
						}
						break
					}
				}
			}

			// å¦‚æœæ’åºæˆåŠŸï¼Œé‡æ–°åºåˆ—åŒ–YAMLå¹¶æ›¿æ¢data
			if shouldRewrite {
				if reorderedData, err := MarshalYAMLWithIndent(&yamlNode); err == nil {
					fixed := RemoveUnicodeEscapeQuotes(string(reorderedData))
					data = []byte(fixed)
					logger.Info("[Subscription] Rewrote YAML data with sorted proxies")
				}
			}
		}
	}
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] èŠ‚ç‚¹æ’åºå®Œæˆ", "step", "node_order", "duration_ms", time.Since(stepStart).Milliseconds())

	// æ ¼å¼è½¬æ¢
	stepStart = time.Now()
	// æ ¹æ®å‚æ•°tçš„ç±»å‹è°ƒç”¨substoreçš„è½¬æ¢ä»£ç 
	clientType := strings.TrimSpace(r.URL.Query().Get("t"))
	// é»˜è®¤æµè§ˆå™¨æ‰“å¼€æ—¶ç›´æ¥è¾“å…¥æ–‡æœ¬, ä¸å†ä¸‹è½½é—®å·
	contentType := "text/yaml; charset=utf-8; charset=UTF-8"
	ext := filepath.Ext(filename)
	if ext == "" {
		ext = ".yaml"
	}

	// clash å’Œ clashmeta ç±»å‹ç›´æ¥è¾“å‡ºæºæ–‡ä»¶, ä¸éœ€è¦è½¬æ¢
	if clientType != "" && clientType != "clash" && clientType != "clashmeta" {
		// Convert subscription using substore producers
		convertedData, err := h.convertSubscription(r.Context(), data, clientType)
		if err != nil {
			writeError(w, http.StatusBadRequest, fmt.Errorf("failed to convert subscription for client %s: %w", clientType, err))
			return
		}
		data = convertedData

		// Set content type and extension based on client type
		switch clientType {
		case "surge", "surgemac", "loon", "qx", "surfboard", "shadowrocket":
			// Text-based formats
			contentType = "text/plain; charset=utf-8"
			ext = ".txt"
		case "sing-box":
			// JSON format
			contentType = "application/json; charset=utf-8"
			ext = ".json"
		case "v2ray":
			// Base64 format
			contentType = "text/plain; charset=utf-8"
			ext = ".txt"
		case "uri":
			// URI format
			contentType = "text/plain; charset=utf-8"
			ext = ".txt"
		default:
			// YAML-based formats (clash, clashmeta, stash, shadowrocket, egern)
			contentType = "text/yaml; charset=utf-8"
			ext = ".yaml"
		}
	}
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] æ ¼å¼è½¬æ¢å®Œæˆ", "step", "format_convert", "duration_ms", time.Since(stepStart).Milliseconds(), "client_type", clientType)

	// æµé‡ç»Ÿè®¡è·å–
	stepStart = time.Now()
	// å°è¯•è·å–æµé‡ä¿¡æ¯ï¼Œå¦‚æœæ¢é’ˆæŠ¥é”™åˆ™è·³è¿‡æµé‡ç»Ÿè®¡ï¼Œä¸å½±å“è®¢é˜…è¾“å‡º
	// å¦‚æœå¼€å¯äº†æ¢é’ˆç»‘å®šï¼Œåªç»Ÿè®¡è®¢é˜…æ–‡ä»¶ä¸­ä½¿ç”¨çš„èŠ‚ç‚¹ç»‘å®šçš„æ¢é’ˆæœåŠ¡å™¨æµé‡
	totalLimit, _, totalUsed, err := h.summary.fetchTotals(r.Context(), username, usedProbeServers)
	hasTrafficInfo := err == nil
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] æµé‡ç»Ÿè®¡è·å–å®Œæˆ", "step", "traffic_fetch", "duration_ms", time.Since(stepStart).Milliseconds())

	// ä½¿ç”¨è®¢é˜…åç§°
	attachmentName := url.PathEscape(displayName)

	// YAML é‡æ’åº
	stepStart = time.Now()
	// å¯¹äº YAML æ ¼å¼çš„æ•°æ®ï¼Œé‡æ–°æ’åºä»¥å°† rule-providers æ”¾åœ¨æœ€å
	// æ³¨æ„ï¼šèŠ‚ç‚¹æ’åºå·²ç»åœ¨è½¬æ¢ä¹‹å‰å®Œæˆï¼Œè¿™é‡Œåªå¤„ç†å…¶ä»–çš„YAMLé‡æ’éœ€æ±‚
	if contentType == "text/yaml; charset=utf-8" || contentType == "text/yaml; charset=utf-8; charset=UTF-8" {
		// ä½¿ç”¨ yaml.Node æ¥ä¿æŒåŸå§‹ç±»å‹ä¿¡æ¯ï¼ˆé¿å… 563905e2 è¢«è§£æä¸ºç§‘å­¦è®¡æ•°æ³•ï¼‰
		var yamlNode yaml.Node
		if err := yaml.Unmarshal(data, &yamlNode); err == nil {
			// æ£€æŸ¥æ˜¯å¦æœ‰ rule-providers éœ€è¦é‡æ–°æ’åº
			// yamlNode.Content[0] æ˜¯æ–‡æ¡£èŠ‚ç‚¹ï¼ŒyamlNode.Content[0].Content æ˜¯æ ¹æ˜ å°„çš„é”®å€¼å¯¹
			if len(yamlNode.Content) > 0 && yamlNode.Content[0].Kind == yaml.MappingNode {
				rootMap := yamlNode.Content[0]

				// æ³¨æ„ï¼šèŠ‚ç‚¹æ’åºå·²ç»åœ¨è½¬æ¢ä¹‹å‰å®Œæˆï¼Œè¿™é‡Œä¸å†é‡å¤æ’åº
				// åªå¤„ç† WireGuard ä¿®å¤å’Œå­—æ®µé‡æ’

				// é‡æ–°æ’åº proxies ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„å­—æ®µ
				for i := 0; i < len(rootMap.Content); i += 2 {
					if rootMap.Content[i].Value == "proxies" {
						proxiesNode := rootMap.Content[i+1]
						if proxiesNode.Kind == yaml.SequenceNode {
							// å…ˆä¿®å¤ WireGuard èŠ‚ç‚¹çš„ allowed-ips å­—æ®µ
							fixWireGuardAllowedIPs(proxiesNode)
							reorderProxies(proxiesNode)
						}
						break
					}
				}

				// é‡æ–°æ’åº proxy-groups ä¸­æ¯ä¸ªä»£ç†ç»„çš„å­—æ®µ
				for i := 0; i < len(rootMap.Content); i += 2 {
					if rootMap.Content[i].Value == "proxy-groups" {
						proxyGroupsNode := rootMap.Content[i+1]
						if proxyGroupsNode.Kind == yaml.SequenceNode {
							reorderProxyGroups(proxyGroupsNode)
						}
						break
					}
				}

				// æŸ¥æ‰¾ rule-providers çš„ä½ç½®
				ruleProvidersIdx := -1
				for i := 0; i < len(rootMap.Content); i += 2 {
					if rootMap.Content[i].Value == "rule-providers" {
						ruleProvidersIdx = i
						break
					}
				}

				// å¦‚æœæ‰¾åˆ° rule-providers ä¸”ä¸åœ¨æœ€åï¼Œåˆ™ç§»åŠ¨åˆ°æœ€å
				if ruleProvidersIdx >= 0 && ruleProvidersIdx < len(rootMap.Content)-2 {
					// æå– rule-providers çš„é”®å’Œå€¼
					keyNode := rootMap.Content[ruleProvidersIdx]
					valueNode := rootMap.Content[ruleProvidersIdx+1]

					// ä»åŸä½ç½®åˆ é™¤
					rootMap.Content = append(rootMap.Content[:ruleProvidersIdx], rootMap.Content[ruleProvidersIdx+2:]...)

					// æ·»åŠ åˆ°æœ€å
					rootMap.Content = append(rootMap.Content, keyNode, valueNode)
				}
			}

			// é‡æ–°åºåˆ—åŒ–ä¸º YAML (ä½¿ç”¨2ç©ºæ ¼ç¼©è¿›)
			if reorderedData, err := MarshalYAMLWithIndent(&yamlNode); err == nil {
				// Fix emoji escapes and quoted numbers
				fixed := RemoveUnicodeEscapeQuotes(string(reorderedData))
				data = []byte(fixed)
			}
		}
	}
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] YAML é‡æ’åºå®Œæˆ", "step", "yaml_reorder", "duration_ms", time.Since(stepStart).Milliseconds())

	w.Header().Set("Content-Type", contentType)
	// åªæœ‰åœ¨æœ‰æµé‡ä¿¡æ¯æ—¶æ‰æ·»åŠ  subscription-userinfo å¤´
	if hasTrafficInfo || externalTrafficLimit > 0 {
		var finalLimit, finalUsed int64

		// åˆ¤æ–­æ˜¯å¦éœ€è¦åŒ…å«æ¢é’ˆæµé‡ï¼š
		// 1. æ¢é’ˆæœåŠ¡å™¨ç»‘å®šå…³é—­æ—¶ï¼Œå§‹ç»ˆåŒ…å«æ¢é’ˆæµé‡
		// 2. æ¢é’ˆæœåŠ¡å™¨ç»‘å®šå¼€å¯æ—¶ï¼Œåªæœ‰ä½¿ç”¨äº†æ¢é’ˆèŠ‚ç‚¹æ‰åŒ…å«æ¢é’ˆæµé‡
		includeProbeTraffic := !probeBindingEnabled || usesProbeNodes

		if includeProbeTraffic && hasTrafficInfo {
			finalLimit = totalLimit + externalTrafficLimit
			finalUsed = totalUsed + externalTrafficUsed
			logger.Info("[Subscription] æœ€ç»ˆæµé‡ç»Ÿè®¡", "user", username)
			logger.Info("[Subscription] æ¢é’ˆæµé‡", "limit_bytes", totalLimit, "limit_gb", float64(totalLimit)/(1024*1024*1024), "used_bytes", totalUsed, "used_gb", float64(totalUsed)/(1024*1024*1024))
		} else {
			// ä»…ç»Ÿè®¡å¤–éƒ¨è®¢é˜…æµé‡
			finalLimit = externalTrafficLimit
			finalUsed = externalTrafficUsed
			logger.Info("[Subscription] æœ€ç»ˆæµé‡ç»Ÿè®¡(ä»…å¤–éƒ¨è®¢é˜…)", "user", username)
			logger.Info("[Subscription] æ¢é’ˆæµé‡æœªåŒ…å«(æ¢é’ˆç»‘å®šå·²å¼€å¯ä½†æœªä½¿ç”¨æ¢é’ˆèŠ‚ç‚¹)")
		}

		logger.Info("[Subscription] å¤–éƒ¨è®¢é˜…æµé‡", "limit_bytes", externalTrafficLimit, "limit_gb", float64(externalTrafficLimit)/(1024*1024*1024), "used_bytes", externalTrafficUsed, "used_gb", float64(externalTrafficUsed)/(1024*1024*1024))
		logger.Info("[Subscription] æ€»æµé‡", "limit_bytes", finalLimit, "limit_gb", float64(finalLimit)/(1024*1024*1024), "used_bytes", finalUsed, "used_gb", float64(finalUsed)/(1024*1024*1024))

		var expireAt *time.Time
		if hasSubscribeFile {
			expireAt = subscribeFile.ExpireAt
		}
		headerValue := buildSubscriptionHeader(finalLimit, finalUsed, expireAt)
		w.Header().Set("subscription-userinfo", headerValue)
		logger.Info("[Subscription] è®¾ç½®è®¢é˜…ç”¨æˆ·ä¿¡æ¯å¤´", "header", headerValue)
	}
	w.Header().Set("profile-update-interval", "24")
	// åªæœ‰éæµè§ˆå™¨è®¿é—®æ—¶æ‰æ·»åŠ  content-disposition å¤´ï¼ˆé¿å…æµè§ˆå™¨ç›´æ¥ä¸‹è½½ï¼‰
	userAgent := r.Header.Get("User-Agent")
	isBrowser := strings.Contains(userAgent, "Mozilla") || strings.Contains(userAgent, "Chrome") || strings.Contains(userAgent, "Safari") || strings.Contains(userAgent, "Edge")
	if !isBrowser {
		w.Header().Set("content-disposition", "attachment;filename*=UTF-8''"+attachmentName)
	}
	w.WriteHeader(http.StatusOK)
	_, _ = w.Write(data)
	logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] è¯·æ±‚å¤„ç†å®Œæˆ", "total_duration_ms", time.Since(requestStart).Milliseconds(), "username", username, "filename", filename)
}

func (h *SubscriptionHandler) resolveSubscription(ctx context.Context, name string) (storage.SubscriptionLink, error) {
	if h == nil {
		return storage.SubscriptionLink{}, errors.New("subscription handler not initialized")
	}

	if h.repo == nil {
		return storage.SubscriptionLink{}, errors.New("subscription repository not configured")
	}

	trimmed := strings.TrimSpace(name)
	if trimmed != "" {
		return h.repo.GetSubscriptionByName(ctx, trimmed)
	}

	if h.fallback != "" {
		link, err := h.repo.GetSubscriptionByName(ctx, h.fallback)
		if err == nil {
			return link, nil
		}
		if !errors.Is(err, storage.ErrSubscriptionNotFound) {
			return storage.SubscriptionLink{}, err
		}
	}

	return h.repo.GetFirstSubscriptionLink(ctx)
}

func buildSubscriptionHeader(totalLimit, totalUsed int64, expireAt *time.Time) string {
	download := strconv.FormatInt(totalUsed, 10)
	total := strconv.FormatInt(totalLimit, 10)
	expire := ""
	if expireAt != nil {
		expire = strconv.FormatInt(expireAt.Unix(), 10)
	}
	return "upload=0; download=" + download + "; total=" + total + "; expire=" + expire
}

// getKeys returns the keys of a map as a slice
func getKeys(m map[string]bool) []string {
	keys := make([]string, 0, len(m))
	for k := range m {
		keys = append(keys, k)
	}
	return keys
}

// GetExternalSubscriptionsFromFile extracts external subscription URLs from YAML file content
// by analyzing proxies and querying the database for their raw_url (external subscription links)
// Also checks proxy-providers for proxy provider configs that reference external subscriptions
func GetExternalSubscriptionsFromFile(ctx context.Context, data []byte, username string, repo *storage.TrafficRepository) (map[string]bool, error) {
	usedURLs := make(map[string]bool)

	// Parse YAML content
	var yamlContent map[string]any
	if err := yaml.Unmarshal(data, &yamlContent); err != nil {
		return usedURLs, fmt.Errorf("failed to parse YAML: %w", err)
	}

	// Extract proxies and query database for their raw_url
	if proxies, ok := yamlContent["proxies"].([]any); ok {
		logger.Info("[Subscription] æ‰¾åˆ°è®¢é˜…æ–‡ä»¶ä¸­çš„ä»£ç†èŠ‚ç‚¹", "count", len(proxies))

		// Collect all proxy names
		proxyNames := make(map[string]bool)
		for _, proxy := range proxies {
			if proxyMap, ok := proxy.(map[string]any); ok {
				if name, ok := proxyMap["name"].(string); ok && name != "" {
					proxyNames[name] = true
				}
			}
		}

		if len(proxyNames) > 0 {
			logger.Info("[Subscription] æŸ¥è¯¢æ•°æ®åº“è·å–å¤–éƒ¨è®¢é˜…URL", "proxy_count", len(proxyNames))

			// Query database for nodes with these names
			nodes, err := repo.ListNodes(ctx, username)
			if err != nil {
				logger.Info("[Subscription] æŸ¥è¯¢èŠ‚ç‚¹åˆ—è¡¨å¤±è´¥", "error", err)
				return usedURLs, fmt.Errorf("failed to list nodes: %w", err)
			}

			// æ”¶é›†ä½¿ç”¨åˆ°çš„å¤–éƒ¨è®¢é˜…æ ‡ç­¾ï¼ˆèŠ‚ç‚¹çš„ Tag å­—æ®µï¼‰
			usedTags := make(map[string]bool)

			// Find matching nodes and collect their raw_url and tags
			for _, node := range nodes {
				if proxyNames[node.NodeName] {
					// å¦‚æœèŠ‚ç‚¹æœ‰ RawURLï¼Œç›´æ¥ä½¿ç”¨
					if node.RawURL != "" {
						usedURLs[node.RawURL] = true
						logger.Info("[Subscription] ä»èŠ‚ç‚¹æ‰¾åˆ°å¤–éƒ¨è®¢é˜…URL", "node_name", node.NodeName, "url", node.RawURL)
					}
					// å¦‚æœèŠ‚ç‚¹æœ‰ Tagï¼ˆå¤–éƒ¨è®¢é˜…åç§°ï¼‰ï¼Œè®°å½•ä¸‹æ¥
					if node.Tag != "" && node.Tag != "æ‰‹åŠ¨è¾“å…¥" {
						usedTags[node.Tag] = true
						logger.Info("[Subscription] èŠ‚ç‚¹æ¥è‡ªå¤–éƒ¨è®¢é˜…", "node_name", node.NodeName, "tag", node.Tag)
					}
				}
			}

			// å¦™å¦™å±‹æ¨¡å¼ï¼šé€šè¿‡èŠ‚ç‚¹çš„ Tagï¼ˆå¤–éƒ¨è®¢é˜…åç§°ï¼‰æ‰¾åˆ°å¤–éƒ¨è®¢é˜…URL
			if len(usedTags) > 0 {
				logger.Info("[Subscription] å‘ç°ä½¿ç”¨å¤–éƒ¨è®¢é˜…çš„èŠ‚ç‚¹", "tag_count", len(usedTags))

				// è·å–æ‰€æœ‰å¤–éƒ¨è®¢é˜…
				externalSubs, err := repo.ListExternalSubscriptions(ctx, username)
				if err != nil {
					logger.Info("[Subscription] è·å–å¤–éƒ¨è®¢é˜…åˆ—è¡¨å¤±è´¥", "error", err)
				} else {
					// æ ¹æ® Tagï¼ˆå¤–éƒ¨è®¢é˜…åç§°ï¼‰æ‰¾åˆ°å¯¹åº”çš„ URL
					for _, sub := range externalSubs {
						if usedTags[sub.Name] {
							usedURLs[sub.URL] = true
							logger.Info("[Subscription] ä»èŠ‚ç‚¹Tagæ‰¾åˆ°å¤–éƒ¨è®¢é˜…URL", "tag", sub.Name, "url", sub.URL)
						}
					}
				}
			}
		}
	}

	// Also check proxy-groups for 'use' field referencing proxy provider configs
	// This handles the case where proxy-providers + use is used instead of direct proxies
	if proxyGroups, ok := yamlContent["proxy-groups"].([]any); ok {
		logger.Info("[Subscription] æ£€æŸ¥ proxy-groups", "group_count", len(proxyGroups))
		providerNames := make(map[string]bool)
		groupNames := make(map[string]bool) // å¦™å¦™å±‹æ¨¡å¼ï¼šæ”¶é›† proxy-group çš„åç§°
		for _, group := range proxyGroups {
			if groupMap, ok := group.(map[string]any); ok {
				// æ”¶é›† proxy-group åç§°ï¼ˆå¦™å¦™å±‹æ¨¡å¼ä¼šåˆ›å»ºåŒåçš„ proxy-groupï¼‰
				if groupName, ok := groupMap["name"].(string); ok && groupName != "" {
					groupNames[groupName] = true
				}

				// æ”¶é›† use å­—æ®µä¸­çš„ provider åç§°ï¼ˆå®¢æˆ·ç«¯æ¨¡å¼ï¼‰
				if useList, ok := groupMap["use"].([]any); ok {
					for _, use := range useList {
						if useName, ok := use.(string); ok && useName != "" {
							providerNames[useName] = true
							logger.Info("[Subscription] æ‰¾åˆ° proxy-group ä½¿ç”¨çš„ provider", "provider_name", useName)
						}
					}
				}
			}
		}

		// åˆå¹¶ä¸¤ç§æ¨¡å¼çš„åç§°
		allNames := make(map[string]bool)
		for name := range providerNames {
			allNames[name] = true
		}
		for name := range groupNames {
			allNames[name] = true
		}

		if len(allNames) > 0 {
			logger.Info("[Subscription] æ‰¾åˆ°ä»£ç†é›†åˆå¼•ç”¨", "count", len(allNames), "from_use", len(providerNames), "from_groups", len(groupNames))

			// Get all proxy provider configs for this user
			configs, err := repo.ListProxyProviderConfigs(ctx, username)
			if err != nil {
				logger.Info("[Subscription] æŸ¥è¯¢ä»£ç†é›†åˆé…ç½®å¤±è´¥", "error", err)
			} else {
				logger.Info("[Subscription] æŸ¥è¯¢åˆ°ç”¨æˆ·çš„ä»£ç†é›†åˆé…ç½®", "count", len(configs))
				// Get external subscriptions to map config -> URL
				externalSubs, err := repo.ListExternalSubscriptions(ctx, username)
				if err != nil {
					logger.Info("[Subscription] è·å–å¤–éƒ¨è®¢é˜…åˆ—è¡¨å¤±è´¥", "error", err)
				} else {
					logger.Info("[Subscription] æŸ¥è¯¢åˆ°ç”¨æˆ·çš„å¤–éƒ¨è®¢é˜…", "count", len(externalSubs))
					// Build external subscription ID -> URL map
					subIDToURL := make(map[int64]string)
					for _, sub := range externalSubs {
						subIDToURL[sub.ID] = sub.URL
					}

					// Find configs that match the names and get their external subscription URLs
					for _, config := range configs {
						logger.Info("[Subscription] æ£€æŸ¥é…ç½®", "config_name", config.Name, "external_sub_id", config.ExternalSubscriptionID, "process_mode", config.ProcessMode)
						if allNames[config.Name] {
							if url, ok := subIDToURL[config.ExternalSubscriptionID]; ok {
								usedURLs[url] = true
								logger.Info("[Subscription] ä»ä»£ç†é›†åˆé…ç½®æ‰¾åˆ°å¤–éƒ¨è®¢é˜…URL", "config_name", config.Name, "mode", config.ProcessMode, "url", url)
							} else {
								logger.Info("[Subscription] é…ç½®çš„å¤–éƒ¨è®¢é˜…IDæœªæ‰¾åˆ°å¯¹åº”URL", "config_name", config.Name, "external_sub_id", config.ExternalSubscriptionID)
							}
						}
					}
				}
			}
		} else {
			logger.Info("[Subscription] proxy-groups ä¸­æœªæ‰¾åˆ°å¼•ç”¨")
		}
	} else {
		logger.Info("[Subscription] YAML ä¸­æœªæ‰¾åˆ° proxy-groups")
	}

	// æ£€æŸ¥ proxy-providers éƒ¨åˆ†ï¼ˆç”¨äºå®¢æˆ·ç«¯æ¨¡å¼çš„ä»£ç†é›†åˆé…ç½®ï¼‰
	// å½“å¤„ç†æ¨¡å¼ä¸ºå®¢æˆ·ç«¯æ¨¡å¼æ—¶ï¼ŒYAML æ–‡ä»¶ä¸­åŒ…å« proxy-providers é…ç½®ï¼ŒURL ä¸ºå†…éƒ¨ API ç«¯ç‚¹
	if proxyProviders, ok := yamlContent["proxy-providers"].(map[string]any); ok {
		logger.Info("[Subscription] æ‰¾åˆ° proxy-providers é…ç½®", "count", len(proxyProviders))

		// æ„å»ºé…ç½® ID -> å¤–éƒ¨è®¢é˜… URL æ˜ å°„
		configIDToURL := make(map[int64]string)
		configs, err := repo.ListProxyProviderConfigs(ctx, username)
		if err == nil {
			externalSubs, err := repo.ListExternalSubscriptions(ctx, username)
			if err == nil {
				// æ„å»ºå¤–éƒ¨è®¢é˜… ID -> URL æ˜ å°„
				subIDToURL := make(map[int64]string)
				for _, sub := range externalSubs {
					subIDToURL[sub.ID] = sub.URL
				}
				// å°†é…ç½® ID æ˜ å°„åˆ°å¤–éƒ¨è®¢é˜… URL
				for _, config := range configs {
					if url, ok := subIDToURL[config.ExternalSubscriptionID]; ok {
						configIDToURL[config.ID] = url
					}
				}
			}
		}

		// è§£ææ¯ä¸ª provider çš„ URLï¼ŒæŸ¥æ‰¾å†…éƒ¨ API ç«¯ç‚¹
		for providerName, provider := range proxyProviders {
			if providerMap, ok := provider.(map[string]any); ok {
				if urlStr, ok := providerMap["url"].(string); ok && urlStr != "" {
					// æ£€æŸ¥æ˜¯å¦ä¸ºå†…éƒ¨ API ç«¯ç‚¹ï¼š/api/proxy-provider/{id}
					if configIDStr, found := strings.CutPrefix(urlStr, "/api/proxy-provider/"); found {
						if configID, err := strconv.ParseInt(configIDStr, 10, 64); err == nil {
							if url, ok := configIDToURL[configID]; ok {
								usedURLs[url] = true
								logger.Info("[Subscription] ä» proxy-providers æ‰¾åˆ°å¤–éƒ¨è®¢é˜…URL",
									"provider_name", providerName, "config_id", configID, "url", url)
							}
						}
					}
				}
			}
		}
	}

	logger.Info("[Subscription] æ‰¾åˆ°å½“å‰æ–‡ä»¶å¼•ç”¨çš„å¤–éƒ¨è®¢é˜…URL", "count", len(usedURLs))
	return usedURLs, nil
}

// syncReferencedExternalSubscriptions syncs only the specified external subscriptions
func syncReferencedExternalSubscriptions(ctx context.Context, repo *storage.TrafficRepository, subscribeDir, username string, subsToSync []storage.ExternalSubscription) error {
	if repo == nil || username == "" || len(subsToSync) == 0 {
		return fmt.Errorf("invalid parameters")
	}

	// Get user settings to check match rule
	userSettings, err := repo.GetUserSettings(ctx, username)
	if err != nil {
		// If settings not found, use default match rule
		userSettings.MatchRule = "node_name"
	}

	logger.Info("[Subscription] ç”¨æˆ·éœ€è¦åŒæ­¥çš„å¤–éƒ¨è®¢é˜…", "user", username, "count", len(subsToSync), "match_rule", userSettings.MatchRule)

	client := &http.Client{
		Timeout: 30 * time.Second,
	}

	// Track total nodes synced
	totalNodesSynced := 0

	for _, sub := range subsToSync {
		subSyncStart := time.Now()
		nodeCount, updatedSub, err := syncSingleExternalSubscription(ctx, client, repo, subscribeDir, username, sub, userSettings)
		if err != nil {
			logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] åŒæ­¥è®¢é˜…å¤±è´¥", "name", sub.Name, "url", sub.URL, "error", err, "duration_ms", time.Since(subSyncStart).Milliseconds())
			continue
		}

		totalNodesSynced += nodeCount

		// Update last sync time and node count
		// Use updatedSub which contains traffic info from parseAndUpdateTrafficInfo
		now := time.Now()
		updatedSub.LastSyncAt = &now
		updatedSub.NodeCount = nodeCount
		if err := repo.UpdateExternalSubscription(ctx, updatedSub); err != nil {
			logger.Info("[Subscription] æ›´æ–°è®¢é˜…åŒæ­¥æ—¶é—´å¤±è´¥", "name", sub.Name, "error", err)
		}
		logger.Info("[â±ï¸ è€—æ—¶ç›‘æµ‹] å¤–éƒ¨è®¢é˜…åŒæ­¥å®Œæˆ", "name", sub.Name, "node_count", nodeCount, "duration_ms", time.Since(subSyncStart).Milliseconds())
	}

	logger.Info("[Subscription] åŒæ­¥å®Œæˆ", "total_nodes", totalNodesSynced, "subscription_count", len(subsToSync))

	// åŒæ­¥å®Œæˆåï¼Œå¤±æ•ˆç›¸å…³ç¼“å­˜ï¼š
	// 1. å¤±æ•ˆå¤–éƒ¨è®¢é˜…å†…å®¹ç¼“å­˜ï¼ˆproxy_provider_serve.go ä¸­çš„ 5 åˆ†é’Ÿç¼“å­˜ï¼‰
	// 2. å¤±æ•ˆä»£ç†é›†åˆèŠ‚ç‚¹ç¼“å­˜
	// è¿™æ ·ä¸‹æ¬¡è·å–è®¢é˜…æ—¶ä¼šä½¿ç”¨æœ€æ–°çš„èŠ‚ç‚¹æ•°æ®
	syncedSubIDs := make(map[int64]bool)
	syncedSubURLs := make(map[string]bool)
	for _, sub := range subsToSync {
		syncedSubIDs[sub.ID] = true
		syncedSubURLs[sub.URL] = true
	}

	// å¤±æ•ˆå¤–éƒ¨è®¢é˜…å†…å®¹ç¼“å­˜
	for url := range syncedSubURLs {
		InvalidateSubscriptionContentCache(url)
		logger.Info("[Subscription] å¤±æ•ˆå¤–éƒ¨è®¢é˜…å†…å®¹ç¼“å­˜", "url", url)
	}

	// è·å–æ‰€æœ‰ä»£ç†é›†åˆé…ç½®ï¼Œå¤±æ•ˆå¼•ç”¨äº†è¿™äº›å¤–éƒ¨è®¢é˜…çš„ä»£ç†é›†åˆç¼“å­˜
	configs, err := repo.ListProxyProviderConfigs(ctx, username)
	if err == nil {
		cache := GetProxyProviderCache()
		invalidatedCount := 0
		for _, config := range configs {
			// æ£€æŸ¥æ˜¯å¦å¼•ç”¨äº†åˆšåˆšåŒæ­¥çš„å¤–éƒ¨è®¢é˜…
			if syncedSubIDs[config.ExternalSubscriptionID] {
				cache.Delete(config.ID)
				invalidatedCount++
				logger.Info("[Subscription] å¤±æ•ˆä»£ç†é›†åˆç¼“å­˜", "config_name", config.Name, "config_id", config.ID)
			}
		}
		if invalidatedCount > 0 {
			logger.Info("[Subscription] ä»£ç†é›†åˆç¼“å­˜å¤±æ•ˆå®Œæˆ", "count", invalidatedCount)
		}
	} else {
		logger.Info("[Subscription] è·å–ä»£ç†é›†åˆé…ç½®å¤±è´¥ï¼Œæ— æ³•å¤±æ•ˆç¼“å­˜", "error", err)
	}

	return nil
}

func (h *SubscriptionHandler) loadTokenInvalidContent() []byte {
	tokenPath := filepath.Join("data", tokenInvalidFilename)
	data, err := os.ReadFile(tokenPath)
	if err != nil {
		logger.Info("[Token Invalid] è¯»å–data/token_invalid.yamlå¤±è´¥ï¼Œä½¿ç”¨å†…ç½®é»˜è®¤å†…å®¹", "path", tokenPath, "error", err)
		return []byte(tokenInvalidYAML)
	}
	if len(data) == 0 {
		logger.Info("[Token Invalid] data/token_invalid.yamlä¸ºç©ºï¼Œä½¿ç”¨å†…ç½®é»˜è®¤å†…å®¹", "path", tokenPath)
		return []byte(tokenInvalidYAML)
	}
	logger.Info("[Token Invalid] ä½¿ç”¨è‡ªå®šä¹‰token_invalid.yaml", "path", tokenPath)
	return data
}

// serveTokenInvalidResponse serves the token invalid YAML content with client type conversion
func (h *SubscriptionHandler) serveTokenInvalidResponse(w http.ResponseWriter, r *http.Request) {
	data := h.loadTokenInvalidContent()

	// æ ¹æ®å‚æ•°tçš„ç±»å‹è°ƒç”¨substoreçš„è½¬æ¢ä»£ç 
	clientType := strings.TrimSpace(r.URL.Query().Get("t"))
	contentType := "text/yaml; charset=utf-8"
	ext := ".yaml"

	// å¦‚æœæŒ‡å®šäº†å®¢æˆ·ç«¯ç±»å‹ä¸”ä¸æ˜¯clash/clashmetaï¼Œè¿›è¡Œè½¬æ¢
	if clientType != "" && clientType != "clash" && clientType != "clashmeta" {
		convertedData, err := h.convertSubscription(r.Context(), data, clientType)
		if err != nil {
			// è½¬æ¢å¤±è´¥ï¼Œè®°å½•æ—¥å¿—ä½†ç»§ç»­è¿”å›YAML
			logger.Info("[Token Invalid] è½¬æ¢å¤±è´¥", "client_type", clientType, "error", err)
		} else {
			data = convertedData

			// æ ¹æ®å®¢æˆ·ç«¯ç±»å‹è®¾ç½®content typeå’Œæ‰©å±•å
			switch clientType {
			case "surge", "surgemac", "loon", "qx", "surfboard", "shadowrocket":
				contentType = "text/plain; charset=utf-8"
				ext = ".txt"
			case "sing-box":
				contentType = "application/json; charset=utf-8"
				ext = ".json"
			case "v2ray", "uri":
				contentType = "text/plain; charset=utf-8"
				ext = ".txt"
			default:
				contentType = "text/yaml; charset=utf-8"
				ext = ".yaml"
			}
		}
	}

	attachmentName := url.PathEscape("Tokenå·²å¤±æ•ˆ" + ext)

	w.Header().Set("Content-Type", contentType)
	w.Header().Set("profile-update-interval", "24")
	if clientType == "" {
		w.Header().Set("content-disposition", "attachment;filename*=UTF-8''"+attachmentName)
	}
	w.WriteHeader(http.StatusOK)
	_, _ = w.Write(data)

	logger.Info("[Token Invalid] è¿”å›Tokenå¤±æ•ˆå“åº”", "client_type", clientType)
}

// convertSubscription converts a YAML subscription file to the specified client format
func (h *SubscriptionHandler) convertSubscription(ctx context.Context, yamlData []byte, clientType string) ([]byte, error) {
	// ä½¿ç”¨ yaml.Node è§£æ, è§£å†³å€¼å‰å¯¼é›¶çš„é—®é¢˜
	var rootNode yaml.Node
	if err := yaml.Unmarshal(yamlData, &rootNode); err != nil {
		return nil, fmt.Errorf("failed to parse YAML: %w", err)
	}

	config, err := yamlNodeToMap(&rootNode)
	if err != nil {
		return nil, fmt.Errorf("failed to convert YAML node: %w", err)
	}

	// è¯»å–yamlä¸­proxieså±æ€§çš„èŠ‚ç‚¹åˆ—è¡¨
	proxiesRaw, ok := config["proxies"]
	if !ok {
		return nil, errors.New("no 'proxies' field found in YAML")
	}

	proxiesArray, ok := proxiesRaw.([]interface{})
	if !ok {
		return nil, errors.New("'proxies' field is not an array")
	}

	// è½¬æ¢æˆsubstoreçš„Proxyç»“æ„
	var proxies []substore.Proxy
	for _, p := range proxiesArray {
		proxyMap, ok := p.(map[string]interface{})
		if !ok {
			continue
		}
		proxies = append(proxies, substore.Proxy(proxyMap))
	}

	if len(proxies) == 0 {
		return nil, errors.New("no valid proxies found in YAML")
	}

	factory := substore.GetDefaultFactory()

	// æ ¹æ®å®¢æˆ·ç«¯ç±»å‹è·å–Producer
	producer, err := factory.GetProducer(clientType)
	if err != nil {
		return nil, fmt.Errorf("unsupported client type '%s': %w", clientType, err)
	}

	// è°ƒç”¨Produceæ–¹æ³•ç”Ÿæˆè½¬æ¢åçš„èŠ‚ç‚¹, ä¼ å…¥å®Œæ•´é…ç½®ä¾›éœ€è¦çš„ Producer ä½¿ç”¨ï¼ˆå¦‚ Stashï¼‰
	// è·å–ç³»ç»Ÿé…ç½®ä»¥è·å–å®¢æˆ·ç«¯å…¼å®¹æ¨¡å¼è®¾ç½®
	systemConfig, _ := h.repo.GetSystemConfig(ctx)
	opts := &substore.ProduceOptions{
		FullConfig:              config,
		ClientCompatibilityMode: systemConfig.ClientCompatibilityMode,
	}
	result, err := producer.Produce(proxies, "", opts)
	if err != nil {
		return nil, fmt.Errorf("failed to produce subscription: %w", err)
	}
	switch v := result.(type) {
	case string:
		return []byte(v), nil
	case []byte:
		return v, nil
	default:
		return nil, fmt.Errorf("unexpected result type from producer: %T, expected string or []byte", result)
	}
}

// fixWireGuardAllowedIPs fixes allowed-ips field type for WireGuard nodes
func fixWireGuardAllowedIPs(proxiesNode *yaml.Node) {
	if proxiesNode == nil || proxiesNode.Kind != yaml.SequenceNode {
		return
	}

	for _, proxyNode := range proxiesNode.Content {
		if proxyNode.Kind != yaml.MappingNode {
			continue
		}

		// Check if this is a WireGuard node
		isWireGuard := false
		for i := 0; i < len(proxyNode.Content); i += 2 {
			if i+1 >= len(proxyNode.Content) {
				break
			}
			if proxyNode.Content[i].Value == "type" && proxyNode.Content[i+1].Value == "wireguard" {
				isWireGuard = true
				break
			}
		}

		if !isWireGuard {
			continue
		}

		// Fix allowed-ips field
		for i := 0; i < len(proxyNode.Content); i += 2 {
			if i+1 >= len(proxyNode.Content) {
				break
			}
			keyNode := proxyNode.Content[i]
			valueNode := proxyNode.Content[i+1]

			if keyNode.Value == "allowed-ips" {
				// If it's already a sequence node, just clear any string tags
				if valueNode.Kind == yaml.SequenceNode {
					valueNode.Tag = ""
					valueNode.Style = 0
					// Also clear tags from child nodes
					for _, childNode := range valueNode.Content {
						if childNode.Tag == "!!str" {
							childNode.Tag = ""
						}
					}
				} else if valueNode.Kind == yaml.ScalarNode {
					// If it's a scalar with !!str tag or looks like a JSON array, clear the tag
					if valueNode.Tag == "!!str" || valueNode.Tag == "tag:yaml.org,2002:str" {
						valueNode.Tag = ""
						valueNode.Style = 0
					}
				}
				break
			}
		}
	}
}

// reorderProxies reorders each proxy's fields in the sequence node
func reorderProxies(seqNode *yaml.Node) {
	if seqNode == nil || seqNode.Kind != yaml.SequenceNode {
		return
	}

	// Process each proxy in the sequence
	for _, proxyNode := range seqNode.Content {
		if proxyNode.Kind == yaml.MappingNode {
			reorderProxyNode(proxyNode)
		}
	}
}

// reorderProxyNode reorders proxy configuration fields
// Priority order: name, type, server, port, then all other fields
func reorderProxyNode(proxyNode *yaml.Node) {
	if proxyNode == nil || proxyNode.Kind != yaml.MappingNode {
		return
	}

	// Priority fields in desired order
	priorityFields := []string{"name", "type", "server", "port"}

	// Create a map of existing fields
	fieldMap := make(map[string]*yaml.Node)
	fieldKeyNodes := make(map[string]*yaml.Node) // Store original key nodes to preserve style
	remainingFields := []*yaml.Node{}

	// Parse existing fields
	for i := 0; i < len(proxyNode.Content); i += 2 {
		if i+1 >= len(proxyNode.Content) {
			break
		}
		keyNode := proxyNode.Content[i]
		valueNode := proxyNode.Content[i+1]

		// Special handling for allowed-ips field to ensure it's treated as an array
		if keyNode.Value == "allowed-ips" && valueNode.Kind == yaml.ScalarNode {
			// If it's a scalar string that looks like a JSON array, mark it explicitly
			if valueNode.Tag == "!!str" || (valueNode.Style == yaml.DoubleQuotedStyle &&
				len(valueNode.Value) > 0 && valueNode.Value[0] == '[') {
				// Remove the !!str tag and let YAML infer the type
				valueNode.Tag = ""
				valueNode.Style = 0
			}
		}

		// Check if this is a priority field
		isPriority := false
		for _, pf := range priorityFields {
			if keyNode.Value == pf {
				fieldMap[pf] = valueNode
				fieldKeyNodes[pf] = keyNode
				isPriority = true
				break
			}
		}

		// If not a priority field, save both key and value for later
		if !isPriority {
			remainingFields = append(remainingFields, keyNode, valueNode)
		}
	}

	// Rebuild the Content with ordered fields
	newContent := []*yaml.Node{}

	// Add priority fields first (in order)
	for _, fieldName := range priorityFields {
		if valueNode, exists := fieldMap[fieldName]; exists {
			// Use original key node if available, otherwise create new one
			keyNode := fieldKeyNodes[fieldName]
			if keyNode == nil {
				keyNode = &yaml.Node{
					Kind:  yaml.ScalarNode,
					Value: fieldName,
				}
			}
			newContent = append(newContent, keyNode, valueNode)
		}
	}

	// Add remaining fields
	newContent = append(newContent, remainingFields...)

	// Replace the original content
	proxyNode.Content = newContent
}

// reorderProxyGroups reorders each proxy group's fields in the sequence node
func reorderProxyGroups(seqNode *yaml.Node) {
	if seqNode == nil || seqNode.Kind != yaml.SequenceNode {
		return
	}

	// Process each proxy group in the sequence
	for _, groupNode := range seqNode.Content {
		if groupNode.Kind == yaml.MappingNode {
			reorderProxyGroupFields(groupNode)
		}
	}
}

// reorderProxyGroupFields reorders proxy group configuration fields
// Priority order: name, type, strategy, proxies, url, interval, tolerance, lazy, hidden
func reorderProxyGroupFields(groupNode *yaml.Node) {
	if groupNode == nil || groupNode.Kind != yaml.MappingNode {
		return
	}

	// Priority fields in desired order
	priorityFields := []string{"name", "type", "strategy", "proxies", "url", "interval", "tolerance", "lazy", "hidden"}

	// Create a map of existing fields
	fieldMap := make(map[string]*yaml.Node)
	remainingFields := []*yaml.Node{}

	// Parse existing fields
	for i := 0; i < len(groupNode.Content); i += 2 {
		if i+1 >= len(groupNode.Content) {
			break
		}
		keyNode := groupNode.Content[i]
		valueNode := groupNode.Content[i+1]

		// Check if this is a priority field
		isPriority := false
		for _, pf := range priorityFields {
			if keyNode.Value == pf {
				fieldMap[pf] = valueNode
				isPriority = true
				break
			}
		}

		// If not a priority field, save both key and value for later
		if !isPriority {
			remainingFields = append(remainingFields, keyNode, valueNode)
		}
	}

	// Rebuild the Content with ordered fields
	newContent := []*yaml.Node{}

	// Add priority fields first (in order)
	for _, fieldName := range priorityFields {
		if valueNode, exists := fieldMap[fieldName]; exists {
			keyNode := &yaml.Node{
				Kind:  yaml.ScalarNode,
				Value: fieldName,
			}
			newContent = append(newContent, keyNode, valueNode)
		}
	}

	// Add remaining fields
	newContent = append(newContent, remainingFields...)

	// Replace the original content
	groupNode.Content = newContent
}

// sortProxiesByNodeOrder æ ¹æ®ç”¨æˆ·é…ç½®çš„èŠ‚ç‚¹é¡ºåºå¯¹ proxies è¿›è¡Œæ’åº
// nodeOrder æ˜¯èŠ‚ç‚¹ ID çš„æ•°ç»„ï¼ŒproxiesNode æ˜¯ YAML ä¸­çš„ proxies åºåˆ—èŠ‚ç‚¹
func sortProxiesByNodeOrder(ctx context.Context, repo *storage.TrafficRepository, username string, proxiesNode *yaml.Node, nodeOrder []int64) error {
	if proxiesNode == nil || proxiesNode.Kind != yaml.SequenceNode {
		return errors.New("invalid proxies node")
	}

	if len(nodeOrder) == 0 || len(proxiesNode.Content) == 0 {
		return nil
	}

	// è·å–ç”¨æˆ·çš„æ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯
	nodes, err := repo.ListNodes(ctx, username)
	if err != nil {
		return fmt.Errorf("failed to list nodes: %w", err)
	}

	// åˆ›å»ºèŠ‚ç‚¹åç§° -> èŠ‚ç‚¹ID çš„æ˜ å°„
	nodeNameToID := make(map[string]int64)
	for _, node := range nodes {
		nodeNameToID[node.NodeName] = node.ID
	}

	// åˆ›å»ºèŠ‚ç‚¹ ID -> æ’åºä½ç½®çš„æ˜ å°„
	nodeIDToPosition := make(map[int64]int)
	for pos, nodeID := range nodeOrder {
		nodeIDToPosition[nodeID] = pos
	}

	// åˆ›å»º proxy èŠ‚ç‚¹çš„æ’åºä¿¡æ¯
	type proxyWithOrder struct {
		node     *yaml.Node
		position int // åœ¨ nodeOrder ä¸­çš„ä½ç½®ï¼Œ-1 è¡¨ç¤ºä¸åœ¨ nodeOrder ä¸­
		name     string
	}

	proxiesWithOrder := make([]proxyWithOrder, 0, len(proxiesNode.Content))

	// è§£ææ¯ä¸ª proxy èŠ‚ç‚¹ï¼Œè·å–å…¶åç§°å’Œæ’åºä½ç½®
	for _, proxyNode := range proxiesNode.Content {
		if proxyNode.Kind != yaml.MappingNode {
			continue
		}

		// æŸ¥æ‰¾ proxy çš„ name å­—æ®µ
		var proxyName string
		for i := 0; i < len(proxyNode.Content); i += 2 {
			if proxyNode.Content[i].Value == "name" {
				if i+1 < len(proxyNode.Content) {
					proxyName = proxyNode.Content[i+1].Value
				}
				break
			}
		}

		if proxyName == "" {
			// å¦‚æœæ²¡æœ‰ name å­—æ®µï¼Œä¿æŒåŸä½ç½®ï¼ˆæ”¾åœ¨æœ€åï¼‰
			proxiesWithOrder = append(proxiesWithOrder, proxyWithOrder{
				node:     proxyNode,
				position: -1,
				name:     "",
			})
			continue
		}

		// æŸ¥æ‰¾è¯¥èŠ‚ç‚¹åç§°å¯¹åº”çš„èŠ‚ç‚¹ ID
		nodeID, exists := nodeNameToID[proxyName]
		position := -1
		if exists {
			// æŸ¥æ‰¾è¯¥èŠ‚ç‚¹ ID åœ¨ nodeOrder ä¸­çš„ä½ç½®
			if pos, found := nodeIDToPosition[nodeID]; found {
				position = pos
			}
		}

		proxiesWithOrder = append(proxiesWithOrder, proxyWithOrder{
			node:     proxyNode,
			position: position,
			name:     proxyName,
		})
	}

	// æ’åºï¼šæŒ‰ position å‡åºæ’åºï¼Œ-1 çš„æ”¾åœ¨æœ€å
	// å¯¹äº position ç›¸åŒçš„èŠ‚ç‚¹ï¼Œä¿æŒåŸæœ‰é¡ºåºï¼ˆç¨³å®šæ’åºï¼‰
	sort.SliceStable(proxiesWithOrder, func(i, j int) bool {
		posI := proxiesWithOrder[i].position
		posJ := proxiesWithOrder[j].position

		// å¦‚æœ i ä¸åœ¨ nodeOrder ä¸­ï¼Œi åº”è¯¥åœ¨ j ä¹‹å
		if posI == -1 {
			return false
		}
		// å¦‚æœ j ä¸åœ¨ nodeOrder ä¸­ï¼Œi åº”è¯¥åœ¨ j ä¹‹å‰
		if posJ == -1 {
			return true
		}
		// éƒ½åœ¨ nodeOrder ä¸­ï¼ŒæŒ‰ position æ’åº
		return posI < posJ
	})

	// æ›´æ–° proxiesNode çš„å†…å®¹
	newContent := make([]*yaml.Node, 0, len(proxiesWithOrder))
	for _, p := range proxiesWithOrder {
		newContent = append(newContent, p.node)
	}
	proxiesNode.Content = newContent

	logger.Info("[Subscription] æŒ‰èŠ‚ç‚¹é¡ºåºæ’åºå®Œæˆ", "count", len(proxiesWithOrder), "user", username)
	return nil
}
